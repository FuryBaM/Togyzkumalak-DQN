{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "16KENFdZAWMBLzIuMYKJdL_NuV3hm4Mlh",
      "authorship_tag": "ABX9TyPz+D65FsJ+nantZ1Fw7qFr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FuryBaM/Togyzkumalak-DQN/blob/master/Togyzkumalak_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZNA7X0-uVDl",
        "outputId": "83281591-5a4c-4122-c676-00a133d40910"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " board (InputLayer)          [(None, 2, 9, 1)]            0         []                            \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)             (None, 2, 9, 81)             162       ['board[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)           (None, 2, 9, 162)            52650     ['conv2d[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)           (None, 2, 9, 162)            105138    ['conv2d_1[0][0]']            \n",
            "                                                                                                  \n",
            " flatten (Flatten)           (None, 2916)                 0         ['conv2d_2[0][0]']            \n",
            "                                                                                                  \n",
            " tuzdyk (InputLayer)         [(None, 2)]                  0         []                            \n",
            "                                                                                                  \n",
            " kazan (InputLayer)          [(None, 2)]                  0         []                            \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 2920)                 0         ['flatten[0][0]',             \n",
            "                                                                     'tuzdyk[0][0]',              \n",
            "                                                                     'kazan[0][0]']               \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 162)                  473202    ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 162)                  0         ['dense[0][0]']               \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 162)                  26406     ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 162)                  26406     ['dense_1[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 162)                  0         ['dense_2[0][0]']             \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 9)                    1467      ['dropout_1[0][0]']           \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 81)                   810       ['dense_3[0][0]']             \n",
            "                                                                                                  \n",
            " dense_5 (Dense)             (None, 81)                   6642      ['dense_4[0][0]']             \n",
            "                                                                                                  \n",
            " dense_6 (Dense)             (None, 18)                   1476      ['dense_5[0][0]']             \n",
            "                                                                                                  \n",
            " dense_7 (Dense)             (None, 9)                    171       ['dense_6[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 694530 (2.65 MB)\n",
            "Trainable params: 694530 (2.65 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " board (InputLayer)          [(None, 2, 9, 1)]            0         []                            \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)           (None, 2, 9, 81)             162       ['board[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)           (None, 2, 9, 162)            52650     ['conv2d_3[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)           (None, 2, 9, 162)            105138    ['conv2d_4[0][0]']            \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)         (None, 2916)                 0         ['conv2d_5[0][0]']            \n",
            "                                                                                                  \n",
            " tuzdyk (InputLayer)         [(None, 2)]                  0         []                            \n",
            "                                                                                                  \n",
            " kazan (InputLayer)          [(None, 2)]                  0         []                            \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate  (None, 2920)                 0         ['flatten_1[0][0]',           \n",
            " )                                                                   'tuzdyk[0][0]',              \n",
            "                                                                     'kazan[0][0]']               \n",
            "                                                                                                  \n",
            " dense_8 (Dense)             (None, 162)                  473202    ['concatenate_1[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)         (None, 162)                  0         ['dense_8[0][0]']             \n",
            "                                                                                                  \n",
            " dense_9 (Dense)             (None, 162)                  26406     ['dropout_2[0][0]']           \n",
            "                                                                                                  \n",
            " dense_10 (Dense)            (None, 162)                  26406     ['dense_9[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)         (None, 162)                  0         ['dense_10[0][0]']            \n",
            "                                                                                                  \n",
            " dense_11 (Dense)            (None, 9)                    1467      ['dropout_3[0][0]']           \n",
            "                                                                                                  \n",
            " dense_12 (Dense)            (None, 81)                   810       ['dense_11[0][0]']            \n",
            "                                                                                                  \n",
            " dense_13 (Dense)            (None, 81)                   6642      ['dense_12[0][0]']            \n",
            "                                                                                                  \n",
            " dense_14 (Dense)            (None, 18)                   1476      ['dense_13[0][0]']            \n",
            "                                                                                                  \n",
            " dense_15 (Dense)            (None, 9)                    171       ['dense_14[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 694530 (2.65 MB)\n",
            "Trainable params: 694530 (2.65 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Target update counter: 0 / 10000.0\n",
            "Saving...\n",
            "Model has saved at 0\n",
            "Total: 12.68 GB\n",
            "Available: 10.44 GB\n",
            "Used: 1.95 GB\n",
            "Percentage usage: 17.7%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7a70dcd48af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7a70dcd48af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11:36:09] Episode: 1/10000, Wins: 0 - 1, Scores: 17 - 88, Epsilon: 0.0, Total reward: -1.5713, Turns: 13\n",
            "Winrate: 0.0\n",
            "memory size: 12\n",
            "[11:36:13] Episode: 2/10000, Wins: 0 - 2, Scores: 12 - 103, Epsilon: 0.0, Total reward: -1.4601000000000002, Turns: 16\n",
            "Winrate: 0.0\n",
            "memory size: 27\n",
            "[11:36:19] Episode: 3/10000, Wins: 0 - 3, Scores: 37 - 82, Epsilon: 0.0, Total reward: -1.3695, Turns: 23\n",
            "Winrate: 0.0\n",
            "memory size: 49\n",
            "[11:36:26] Episode: 4/10000, Wins: 0 - 4, Scores: 30 - 93, Epsilon: 0.0, Total reward: -1.2711000000000001, Turns: 15\n",
            "Winrate: 0.0\n",
            "memory size: 63\n",
            "[11:36:33] Episode: 5/10000, Wins: 0 - 5, Scores: 44 - 82, Epsilon: 0.0, Total reward: -1.2398, Turns: 32\n",
            "Winrate: 0.0\n",
            "memory size: 94\n",
            "[11:36:43] Episode: 6/10000, Wins: 0 - 6, Scores: 54 - 82, Epsilon: 0.0, Total reward: -0.9688000000000001, Turns: 30\n",
            "Winrate: 0.0\n",
            "memory size: 123\n",
            "[11:36:50] Episode: 7/10000, Wins: 0 - 7, Scores: 12 - 88, Epsilon: 0.0, Total reward: -1.4699999999999998, Turns: 18\n",
            "Winrate: 0.0\n",
            "memory size: 140\n",
            "[11:36:58] Episode: 8/10000, Wins: 0 - 8, Scores: 29 - 86, Epsilon: 0.0, Total reward: -1.4511, Turns: 18\n",
            "Winrate: 0.0\n",
            "memory size: 157\n",
            "[11:37:06] Episode: 9/10000, Wins: 0 - 9, Scores: 12 - 88, Epsilon: 0.0, Total reward: -1.4674000000000003, Turns: 22\n",
            "Winrate: 0.0\n",
            "memory size: 178\n",
            "[11:37:13] Episode: 10/10000, Wins: 0 - 10, Scores: 31 - 84, Epsilon: 0.0, Total reward: -1.4289, Turns: 29\n",
            "Winrate: 0.0\n",
            "memory size: 206\n",
            "[11:37:22] Episode: 11/10000, Wins: 0 - 11, Scores: 45 - 82, Epsilon: 0.0, Total reward: -1.0503, Turns: 27\n",
            "Winrate: 0.0\n",
            "memory size: 232\n",
            "[11:37:31] Episode: 12/10000, Wins: 0 - 12, Scores: 46 - 82, Epsilon: 0.0, Total reward: -0.40830000000000005, Turns: 53\n",
            "Winrate: 0.0\n",
            "memory size: 284\n",
            "[11:37:41] Episode: 13/10000, Wins: 0 - 13, Scores: 41 - 82, Epsilon: 0.0, Total reward: -0.5074000000000002, Turns: 54\n",
            "Winrate: 0.0\n",
            "memory size: 337\n",
            "[11:37:50] Episode: 14/10000, Wins: 0 - 14, Scores: 34 - 84, Epsilon: 0.0, Total reward: -1.3496, Turns: 35\n",
            "Winrate: 0.0\n",
            "memory size: 371\n",
            "[11:37:58] Episode: 15/10000, Wins: 0 - 15, Scores: 28 - 82, Epsilon: 0.0, Total reward: -1.2993999999999999, Turns: 28\n",
            "Winrate: 0.0\n",
            "memory size: 398\n",
            "[11:38:06] Episode: 16/10000, Wins: 0 - 16, Scores: 30 - 82, Epsilon: 0.0, Total reward: -1.1716000000000002, Turns: 18\n",
            "Winrate: 0.0\n",
            "memory size: 415\n",
            "[11:38:14] Episode: 17/10000, Wins: 0 - 17, Scores: 26 - 82, Epsilon: 0.0, Total reward: -1.318, Turns: 26\n",
            "Winrate: 0.0\n",
            "memory size: 440\n",
            "[11:38:20] Episode: 18/10000, Wins: 0 - 18, Scores: 17 - 86, Epsilon: 0.0, Total reward: -1.5231, Turns: 13\n",
            "Winrate: 0.0\n",
            "memory size: 452\n",
            "[11:38:26] Episode: 19/10000, Wins: 0 - 19, Scores: 26 - 96, Epsilon: 0.0, Total reward: -1.2718000000000003, Turns: 13\n",
            "Winrate: 0.0\n",
            "memory size: 464\n",
            "[11:38:33] Episode: 20/10000, Wins: 0 - 20, Scores: 22 - 90, Epsilon: 0.0, Total reward: -1.3706, Turns: 12\n",
            "Winrate: 0.0\n",
            "memory size: 475\n",
            "[11:38:39] Episode: 21/10000, Wins: 0 - 21, Scores: 22 - 84, Epsilon: 0.0, Total reward: -1.3106, Turns: 11\n",
            "Winrate: 0.0\n",
            "memory size: 485\n",
            "[11:38:46] Episode: 22/10000, Wins: 0 - 22, Scores: 25 - 92, Epsilon: 0.0, Total reward: -1.4405000000000001, Turns: 14\n",
            "Winrate: 0.0\n",
            "memory size: 498\n",
            "[11:38:53] Episode: 23/10000, Wins: 0 - 23, Scores: 26 - 82, Epsilon: 0.0, Total reward: -1.3702000000000003, Turns: 15\n",
            "Winrate: 0.0\n",
            "memory size: 512\n",
            "[11:39:01] Episode: 24/10000, Wins: 0 - 24, Scores: 19 - 82, Epsilon: 0.0, Total reward: -1.3932999999999998, Turns: 21\n",
            "Winrate: 0.0\n",
            "memory size: 532\n",
            "[11:39:08] Episode: 25/10000, Wins: 0 - 25, Scores: 28 - 89, Epsilon: 0.0, Total reward: -1.3401, Turns: 19\n",
            "Winrate: 0.0\n",
            "memory size: 550\n",
            "Target update counter: 746 / 10000.0\n",
            "Saving...\n",
            "Model has saved at 25\n",
            "Total: 12.68 GB\n",
            "Available: 8.64 GB\n",
            "Used: 3.75 GB\n",
            "Percentage usage: 31.8%\n",
            "[11:39:19] Episode: 26/10000, Wins: 0 - 26, Scores: 30 - 89, Epsilon: 0.0, Total reward: -1.2303, Turns: 29\n",
            "Winrate: 0.0\n",
            "memory size: 578\n",
            "[11:39:27] Episode: 27/10000, Wins: 0 - 27, Scores: 30 - 82, Epsilon: 0.0, Total reward: -1.3678000000000001, Turns: 30\n",
            "Winrate: 0.0\n",
            "memory size: 607\n",
            "[11:39:33] Episode: 28/10000, Wins: 0 - 28, Scores: 12 - 86, Epsilon: 0.0, Total reward: -1.471, Turns: 11\n",
            "Winrate: 0.0\n",
            "memory size: 617\n",
            "[11:39:43] Episode: 29/10000, Wins: 0 - 29, Scores: 30 - 82, Epsilon: 0.0, Total reward: -1.2906, Turns: 43\n",
            "Winrate: 0.0\n",
            "memory size: 659\n",
            "[11:39:50] Episode: 30/10000, Wins: 0 - 30, Scores: 12 - 86, Epsilon: 0.0, Total reward: -1.4595999999999998, Turns: 14\n",
            "Winrate: 0.0\n",
            "memory size: 672\n",
            "[11:39:57] Episode: 31/10000, Wins: 0 - 31, Scores: 30 - 82, Epsilon: 0.0, Total reward: -1.4396000000000002, Turns: 18\n",
            "Winrate: 0.0\n",
            "memory size: 689\n",
            "[11:40:05] Episode: 32/10000, Wins: 0 - 32, Scores: 29 - 84, Epsilon: 0.0, Total reward: -1.4396999999999998, Turns: 24\n",
            "Winrate: 0.0\n",
            "memory size: 712\n",
            "[11:40:14] Episode: 33/10000, Wins: 0 - 33, Scores: 49 - 84, Epsilon: 0.0, Total reward: -1.2389, Turns: 51\n",
            "Winrate: 0.0\n",
            "memory size: 762\n",
            "[11:40:22] Episode: 34/10000, Wins: 0 - 34, Scores: 28 - 84, Epsilon: 0.0, Total reward: -1.3090000000000002, Turns: 14\n",
            "Winrate: 0.0\n",
            "memory size: 775\n",
            "[11:40:32] Episode: 35/10000, Wins: 0 - 35, Scores: 54 - 82, Epsilon: 0.0, Total reward: -1.101, Turns: 52\n",
            "Winrate: 0.0\n",
            "memory size: 826\n",
            "[11:40:39] Episode: 36/10000, Wins: 0 - 36, Scores: 30 - 82, Epsilon: 0.0, Total reward: -1.2273999999999998, Turns: 35\n",
            "Winrate: 0.0\n",
            "memory size: 860\n",
            "[11:40:48] Episode: 37/10000, Wins: 0 - 37, Scores: 40 - 82, Epsilon: 0.0, Total reward: -1.1184, Turns: 31\n",
            "Winrate: 0.0\n",
            "memory size: 890\n",
            "[11:40:53] Episode: 38/10000, Wins: 0 - 38, Scores: 10 - 82, Epsilon: 0.0, Total reward: -1.4930000000000003, Turns: 8\n",
            "Winrate: 0.0\n",
            "memory size: 897\n",
            "[11:41:01] Episode: 39/10000, Wins: 0 - 39, Scores: 22 - 82, Epsilon: 0.0, Total reward: -1.3690000000000002, Turns: 13\n",
            "Winrate: 0.0\n",
            "memory size: 909\n",
            "[11:41:08] Episode: 40/10000, Wins: 0 - 40, Scores: 44 - 82, Epsilon: 0.0, Total reward: -1.2014, Turns: 20\n",
            "Winrate: 0.0\n",
            "memory size: 928\n",
            "[11:41:15] Episode: 41/10000, Wins: 0 - 41, Scores: 27 - 94, Epsilon: 0.0, Total reward: -1.3199, Turns: 12\n",
            "Winrate: 0.0\n",
            "memory size: 939\n",
            "[11:41:23] Episode: 42/10000, Wins: 0 - 42, Scores: 51 - 84, Epsilon: 0.0, Total reward: -1.2293, Turns: 42\n",
            "Winrate: 0.0\n",
            "memory size: 980\n",
            "[11:41:31] Episode: 43/10000, Wins: 0 - 43, Scores: 40 - 86, Epsilon: 0.0, Total reward: -1.3297999999999999, Turns: 23\n",
            "Winrate: 0.0\n",
            "memory size: 1002\n",
            "[11:41:37] Episode: 44/10000, Wins: 0 - 44, Scores: 30 - 82, Epsilon: 0.0, Total reward: -1.2896, Turns: 20\n",
            "Winrate: 0.0\n",
            "memory size: 1021\n",
            "[11:41:46] Episode: 45/10000, Wins: 0 - 45, Scores: 37 - 108, Epsilon: 0.0, Total reward: -1.3597000000000001, Turns: 34\n",
            "Winrate: 0.0\n",
            "memory size: 1054\n",
            "[11:41:54] Episode: 46/10000, Wins: 0 - 46, Scores: 36 - 82, Epsilon: 0.0, Total reward: -1.3690000000000002, Turns: 31\n",
            "Winrate: 0.0\n",
            "memory size: 1084\n",
            "[11:42:01] Episode: 47/10000, Wins: 0 - 47, Scores: 28 - 82, Epsilon: 0.0, Total reward: -1.2878, Turns: 19\n",
            "Winrate: 0.0\n",
            "memory size: 1102\n",
            "[11:42:10] Episode: 48/10000, Wins: 0 - 48, Scores: 33 - 82, Epsilon: 0.0, Total reward: -1.2013, Turns: 32\n",
            "Winrate: 0.0\n",
            "memory size: 1133\n",
            "[11:42:16] Episode: 49/10000, Wins: 0 - 49, Scores: 21 - 82, Epsilon: 0.0, Total reward: -1.3675000000000002, Turns: 16\n",
            "Winrate: 0.0\n",
            "memory size: 1148\n",
            "[11:42:23] Episode: 50/10000, Wins: 0 - 50, Scores: 22 - 82, Epsilon: 0.0, Total reward: -1.5130000000000003, Turns: 16\n",
            "Winrate: 0.0\n",
            "memory size: 1163\n",
            "Target update counter: 1546 / 10000.0\n",
            "Saving...\n",
            "Model has saved at 50\n",
            "Total: 12.68 GB\n",
            "Available: 8.58 GB\n",
            "Used: 3.81 GB\n",
            "Percentage usage: 32.4%\n",
            "[11:42:37] Episode: 51/10000, Wins: 0 - 51, Scores: 48 - 83, Epsilon: 0.0, Total reward: -0.9283000000000001, Turns: 63\n",
            "Winrate: 0.0\n",
            "memory size: 1225\n",
            "[11:42:43] Episode: 52/10000, Wins: 0 - 52, Scores: 34 - 83, Epsilon: 0.0, Total reward: -1.0903, Turns: 14\n",
            "Winrate: 0.0\n",
            "memory size: 1238\n",
            "[11:42:52] Episode: 53/10000, Wins: 0 - 53, Scores: 34 - 82, Epsilon: 0.0, Total reward: -1.439, Turns: 27\n",
            "Winrate: 0.0\n",
            "memory size: 1264\n",
            "[11:42:59] Episode: 54/10000, Wins: 0 - 54, Scores: 26 - 84, Epsilon: 0.0, Total reward: -1.3312, Turns: 22\n",
            "Winrate: 0.0\n",
            "memory size: 1285\n",
            "[11:43:08] Episode: 55/10000, Wins: 0 - 55, Scores: 45 - 85, Epsilon: 0.0, Total reward: -1.17, Turns: 33\n",
            "Winrate: 0.0\n",
            "memory size: 1317\n",
            "[11:43:16] Episode: 56/10000, Wins: 0 - 56, Scores: 29 - 82, Epsilon: 0.0, Total reward: -1.3984999999999999, Turns: 26\n",
            "Winrate: 0.0\n",
            "memory size: 1342\n",
            "[11:43:23] Episode: 57/10000, Wins: 0 - 57, Scores: 29 - 84, Epsilon: 0.0, Total reward: -1.4393000000000002, Turns: 22\n",
            "Winrate: 0.0\n",
            "memory size: 1363\n",
            "[11:43:31] Episode: 58/10000, Wins: 0 - 58, Scores: 12 - 82, Epsilon: 0.0, Total reward: -1.4675999999999998, Turns: 21\n",
            "Winrate: 0.0\n",
            "memory size: 1383\n",
            "[11:43:37] Episode: 59/10000, Wins: 0 - 59, Scores: 14 - 82, Epsilon: 0.0, Total reward: -1.4934, Turns: 10\n",
            "Winrate: 0.0\n",
            "memory size: 1392\n",
            "[11:43:45] Episode: 60/10000, Wins: 0 - 60, Scores: 35 - 83, Epsilon: 0.0, Total reward: -1.4234000000000002, Turns: 26\n",
            "Winrate: 0.0\n",
            "memory size: 1417\n",
            "[11:43:52] Episode: 61/10000, Wins: 0 - 61, Scores: 42 - 82, Epsilon: 0.0, Total reward: -1.2196000000000002, Turns: 37\n",
            "Winrate: 0.0\n",
            "memory size: 1453\n",
            "[11:44:04] Episode: 62/10000, Wins: 1 - 61, Scores: 62 - 74, Epsilon: 0.0, Total reward: 0.8926, Turns: 80\n",
            "Winrate: 0.016129032258064516\n",
            "memory size: 1532\n",
            "[11:44:11] Episode: 63/10000, Wins: 1 - 62, Scores: 12 - 82, Epsilon: 0.0, Total reward: -1.4698000000000002, Turns: 9\n",
            "Winrate: 0.015873015873015872\n",
            "memory size: 1540\n",
            "[11:44:17] Episode: 64/10000, Wins: 1 - 63, Scores: 17 - 83, Epsilon: 0.0, Total reward: -1.5002, Turns: 14\n",
            "Winrate: 0.015625\n",
            "memory size: 1553\n",
            "[11:44:26] Episode: 65/10000, Wins: 1 - 64, Scores: 39 - 82, Epsilon: 0.0, Total reward: -1.2503, Turns: 21\n",
            "Winrate: 0.015384615384615385\n",
            "memory size: 1573\n",
            "[11:44:32] Episode: 66/10000, Wins: 1 - 65, Scores: 12 - 88, Epsilon: 0.0, Total reward: -1.4701999999999997, Turns: 16\n",
            "Winrate: 0.015151515151515152\n",
            "memory size: 1588\n",
            "[11:44:39] Episode: 67/10000, Wins: 1 - 66, Scores: 16 - 90, Epsilon: 0.0, Total reward: -1.5778, Turns: 17\n",
            "Winrate: 0.014925373134328358\n",
            "memory size: 1604\n",
            "[11:44:46] Episode: 68/10000, Wins: 1 - 67, Scores: 17 - 82, Epsilon: 0.0, Total reward: -1.3975, Turns: 21\n",
            "Winrate: 0.014705882352941176\n",
            "memory size: 1624\n",
            "[11:44:55] Episode: 69/10000, Wins: 1 - 68, Scores: 43 - 84, Epsilon: 0.0, Total reward: -1.2878999999999998, Turns: 32\n",
            "Winrate: 0.014492753623188406\n",
            "memory size: 1655\n",
            "[11:45:02] Episode: 70/10000, Wins: 1 - 69, Scores: 33 - 102, Epsilon: 0.0, Total reward: -1.4101, Turns: 25\n",
            "Winrate: 0.014285714285714285\n",
            "memory size: 1679\n",
            "[11:45:09] Episode: 71/10000, Wins: 1 - 70, Scores: 26 - 84, Epsilon: 0.0, Total reward: -1.4572, Turns: 14\n",
            "Winrate: 0.014084507042253521\n",
            "memory size: 1692\n",
            "[11:45:16] Episode: 72/10000, Wins: 1 - 71, Scores: 33 - 84, Epsilon: 0.0, Total reward: -1.3094999999999999, Turns: 22\n",
            "Winrate: 0.013888888888888888\n",
            "memory size: 1713\n",
            "[11:45:25] Episode: 73/10000, Wins: 1 - 72, Scores: 22 - 85, Epsilon: 0.0, Total reward: -1.2597000000000003, Turns: 24\n",
            "Winrate: 0.0136986301369863\n",
            "memory size: 1736\n",
            "[11:45:32] Episode: 74/10000, Wins: 1 - 73, Scores: 30 - 90, Epsilon: 0.0, Total reward: -1.3392000000000002, Turns: 17\n",
            "Winrate: 0.013513513513513514\n",
            "memory size: 1752\n",
            "[11:45:39] Episode: 75/10000, Wins: 1 - 74, Scores: 37 - 82, Epsilon: 0.0, Total reward: -1.2607, Turns: 22\n",
            "Winrate: 0.013333333333333334\n",
            "memory size: 1773\n",
            "Target update counter: 2346 / 10000.0\n",
            "Saving...\n",
            "Model has saved at 75\n",
            "Total: 12.68 GB\n",
            "Available: 8.55 GB\n",
            "Used: 3.84 GB\n",
            "Percentage usage: 32.6%\n",
            "[11:45:52] Episode: 76/10000, Wins: 1 - 75, Scores: 24 - 90, Epsilon: 0.0, Total reward: -1.3508, Turns: 39\n",
            "Winrate: 0.013157894736842105\n",
            "memory size: 1811\n",
            "[11:46:00] Episode: 77/10000, Wins: 1 - 76, Scores: 27 - 86, Epsilon: 0.0, Total reward: -1.3218999999999999, Turns: 17\n",
            "Winrate: 0.012987012987012988\n",
            "memory size: 1827\n",
            "[11:46:06] Episode: 78/10000, Wins: 1 - 77, Scores: 14 - 86, Epsilon: 0.0, Total reward: -1.4394, Turns: 21\n",
            "Winrate: 0.01282051282051282\n",
            "memory size: 1847\n",
            "[11:46:18] Episode: 79/10000, Wins: 1 - 78, Scores: 46 - 82, Epsilon: 0.0, Total reward: -1.0594000000000001, Turns: 59\n",
            "Winrate: 0.012658227848101266\n",
            "memory size: 1905\n",
            "[11:46:24] Episode: 80/10000, Wins: 1 - 79, Scores: 13 - 86, Epsilon: 0.0, Total reward: -1.6070999999999998, Turns: 12\n",
            "Winrate: 0.0125\n",
            "memory size: 1916\n",
            "[11:46:32] Episode: 81/10000, Wins: 1 - 80, Scores: 26 - 82, Epsilon: 0.0, Total reward: -1.3297999999999999, Turns: 28\n",
            "Winrate: 0.012345679012345678\n",
            "memory size: 1943\n",
            "[11:46:39] Episode: 82/10000, Wins: 1 - 81, Scores: 43 - 82, Epsilon: 0.0, Total reward: -1.1488999999999998, Turns: 14\n",
            "Winrate: 0.012195121951219513\n",
            "memory size: 1956\n",
            "[11:46:46] Episode: 83/10000, Wins: 1 - 82, Scores: 15 - 96, Epsilon: 0.0, Total reward: -1.5397000000000003, Turns: 15\n",
            "Winrate: 0.012048192771084338\n",
            "memory size: 1970\n",
            "[11:46:56] Episode: 84/10000, Wins: 1 - 83, Scores: 36 - 82, Epsilon: 0.0, Total reward: -1.2209999999999999, Turns: 40\n",
            "Winrate: 0.011904761904761904\n",
            "memory size: 2009\n",
            "[11:47:03] Episode: 85/10000, Wins: 1 - 84, Scores: 16 - 94, Epsilon: 0.0, Total reward: -1.4796, Turns: 26\n",
            "Winrate: 0.011764705882352941\n",
            "memory size: 2034\n",
            "[11:47:12] Episode: 86/10000, Wins: 1 - 85, Scores: 12 - 82, Epsilon: 0.0, Total reward: -1.4595999999999998, Turns: 21\n",
            "Winrate: 0.011627906976744186\n",
            "memory size: 2048\n",
            "[11:47:18] Episode: 87/10000, Wins: 1 - 86, Scores: 22 - 84, Epsilon: 0.0, Total reward: -1.3494000000000002, Turns: 22\n",
            "Winrate: 0.011494252873563218\n",
            "memory size: 2048\n",
            "[11:47:25] Episode: 88/10000, Wins: 1 - 87, Scores: 16 - 88, Epsilon: 0.0, Total reward: -1.3130000000000002, Turns: 12\n",
            "Winrate: 0.011363636363636364\n",
            "memory size: 2048\n",
            "[11:47:32] Episode: 89/10000, Wins: 1 - 88, Scores: 16 - 88, Epsilon: 0.0, Total reward: -1.4302, Turns: 18\n",
            "Winrate: 0.011235955056179775\n",
            "memory size: 2048\n",
            "[11:47:40] Episode: 90/10000, Wins: 1 - 89, Scores: 24 - 91, Epsilon: 0.0, Total reward: -1.3300999999999998, Turns: 17\n",
            "Winrate: 0.011111111111111112\n",
            "memory size: 2048\n",
            "[11:47:47] Episode: 91/10000, Wins: 1 - 90, Scores: 35 - 82, Epsilon: 0.0, Total reward: -1.2215, Turns: 19\n",
            "Winrate: 0.01098901098901099\n",
            "memory size: 2048\n",
            "[11:47:56] Episode: 92/10000, Wins: 1 - 91, Scores: 28 - 84, Epsilon: 0.0, Total reward: -1.4472, Turns: 33\n",
            "Winrate: 0.010869565217391304\n",
            "memory size: 2048\n",
            "[11:48:04] Episode: 93/10000, Wins: 1 - 92, Scores: 18 - 82, Epsilon: 0.0, Total reward: -1.5625999999999998, Turns: 19\n",
            "Winrate: 0.010752688172043012\n",
            "memory size: 2048\n",
            "[11:48:11] Episode: 94/10000, Wins: 1 - 93, Scores: 35 - 82, Epsilon: 0.0, Total reward: -1.4309, Turns: 23\n",
            "Winrate: 0.010638297872340425\n",
            "memory size: 2048\n",
            "[11:48:19] Episode: 95/10000, Wins: 1 - 94, Scores: 35 - 92, Epsilon: 0.0, Total reward: -1.3577000000000001, Turns: 26\n",
            "Winrate: 0.010526315789473684\n",
            "memory size: 2048\n",
            "[11:48:26] Episode: 96/10000, Wins: 1 - 95, Scores: 30 - 88, Epsilon: 0.0, Total reward: -1.27, Turns: 17\n",
            "Winrate: 0.010416666666666666\n",
            "memory size: 2048\n",
            "[11:48:34] Episode: 97/10000, Wins: 1 - 96, Scores: 14 - 86, Epsilon: 0.0, Total reward: -1.4404000000000003, Turns: 17\n",
            "Winrate: 0.010309278350515464\n",
            "memory size: 2048\n",
            "[11:48:40] Episode: 98/10000, Wins: 1 - 97, Scores: 24 - 84, Epsilon: 0.0, Total reward: -1.4492000000000003, Turns: 18\n",
            "Winrate: 0.01020408163265306\n",
            "memory size: 2048\n",
            "[11:48:48] Episode: 99/10000, Wins: 1 - 98, Scores: 10 - 90, Epsilon: 0.0, Total reward: -1.4913999999999998, Turns: 10\n",
            "Winrate: 0.010101010101010102\n",
            "memory size: 2048\n",
            "[11:48:59] Episode: 100/10000, Wins: 1 - 99, Scores: 24 - 82, Epsilon: 0.0, Total reward: -1.3780000000000001, Turns: 50\n",
            "Winrate: 0.01\n",
            "memory size: 2048\n",
            "Target update counter: 3146 / 10000.0\n",
            "Saving...\n",
            "Model has saved at 100\n",
            "Total: 12.68 GB\n",
            "Available: 8.55 GB\n",
            "Used: 3.84 GB\n",
            "Percentage usage: 32.6%\n",
            "[11:49:10] Episode: 101/10000, Wins: 1 - 100, Scores: 37 - 82, Epsilon: 0.0, Total reward: -1.2103000000000002, Turns: 21\n",
            "Winrate: 0.009900990099009901\n",
            "memory size: 2048\n",
            "[11:49:18] Episode: 102/10000, Wins: 1 - 101, Scores: 30 - 90, Epsilon: 0.0, Total reward: -1.3784, Turns: 27\n",
            "Winrate: 0.00980392156862745\n",
            "memory size: 2048\n",
            "[11:49:26] Episode: 103/10000, Wins: 1 - 102, Scores: 22 - 92, Epsilon: 0.0, Total reward: -1.3701999999999999, Turns: 21\n",
            "Winrate: 0.009708737864077669\n",
            "memory size: 2048\n",
            "[11:49:32] Episode: 104/10000, Wins: 1 - 103, Scores: 22 - 86, Epsilon: 0.0, Total reward: -1.3592000000000002, Turns: 10\n",
            "Winrate: 0.009615384615384616\n",
            "memory size: 2048\n",
            "[11:49:41] Episode: 105/10000, Wins: 1 - 104, Scores: 22 - 96, Epsilon: 0.0, Total reward: -1.5194, Turns: 32\n",
            "Winrate: 0.009523809523809525\n",
            "memory size: 2048\n",
            "[11:49:49] Episode: 106/10000, Wins: 1 - 105, Scores: 34 - 83, Epsilon: 0.0, Total reward: -1.1179000000000001, Turns: 34\n",
            "Winrate: 0.009433962264150943\n",
            "memory size: 2048\n",
            "[11:49:57] Episode: 107/10000, Wins: 1 - 106, Scores: 22 - 82, Epsilon: 0.0, Total reward: -1.3586000000000005, Turns: 16\n",
            "Winrate: 0.009345794392523364\n",
            "memory size: 2048\n",
            "[11:50:06] Episode: 108/10000, Wins: 1 - 107, Scores: 28 - 86, Epsilon: 0.0, Total reward: -1.31, Turns: 33\n",
            "Winrate: 0.009259259259259259\n",
            "memory size: 2048\n",
            "[11:50:13] Episode: 109/10000, Wins: 1 - 108, Scores: 10 - 82, Epsilon: 0.0, Total reward: -1.492, Turns: 11\n",
            "Winrate: 0.009174311926605505\n",
            "memory size: 2048\n",
            "[11:50:20] Episode: 110/10000, Wins: 1 - 109, Scores: 15 - 84, Epsilon: 0.0, Total reward: -1.5911000000000004, Turns: 17\n",
            "Winrate: 0.00909090909090909\n",
            "memory size: 2048\n",
            "[11:50:27] Episode: 111/10000, Wins: 1 - 110, Scores: 25 - 84, Epsilon: 0.0, Total reward: -1.3675000000000002, Turns: 22\n",
            "Winrate: 0.009009009009009009\n",
            "memory size: 2048\n",
            "[11:50:35] Episode: 112/10000, Wins: 1 - 111, Scores: 21 - 82, Epsilon: 0.0, Total reward: -1.4685000000000001, Turns: 22\n",
            "Winrate: 0.008928571428571428\n",
            "memory size: 2048\n",
            "[11:50:41] Episode: 113/10000, Wins: 1 - 112, Scores: 10 - 90, Epsilon: 0.0, Total reward: -1.49, Turns: 14\n",
            "Winrate: 0.008849557522123894\n",
            "memory size: 2048\n",
            "[11:50:50] Episode: 114/10000, Wins: 1 - 113, Scores: 39 - 86, Epsilon: 0.0, Total reward: -1.3899000000000001, Turns: 43\n",
            "Winrate: 0.008771929824561403\n",
            "memory size: 2048\n",
            "[11:50:56] Episode: 115/10000, Wins: 1 - 114, Scores: 17 - 85, Epsilon: 0.0, Total reward: -1.4569999999999999, Turns: 13\n",
            "Winrate: 0.008695652173913044\n",
            "memory size: 2048\n",
            "[11:51:04] Episode: 116/10000, Wins: 1 - 115, Scores: 22 - 86, Epsilon: 0.0, Total reward: -1.3687999999999998, Turns: 13\n",
            "Winrate: 0.008620689655172414\n",
            "memory size: 2048\n",
            "[11:51:11] Episode: 117/10000, Wins: 1 - 116, Scores: 16 - 82, Epsilon: 0.0, Total reward: -1.577, Turns: 19\n",
            "Winrate: 0.008547008547008548\n",
            "memory size: 2048\n",
            "[11:51:21] Episode: 118/10000, Wins: 1 - 117, Scores: 22 - 82, Epsilon: 0.0, Total reward: -1.3474, Turns: 38\n",
            "Winrate: 0.00847457627118644\n",
            "memory size: 2048\n",
            "[11:51:30] Episode: 119/10000, Wins: 1 - 118, Scores: 42 - 83, Epsilon: 0.0, Total reward: -1.3099, Turns: 38\n",
            "Winrate: 0.008403361344537815\n",
            "memory size: 2048\n",
            "[11:51:38] Episode: 120/10000, Wins: 1 - 119, Scores: 21 - 82, Epsilon: 0.0, Total reward: -1.5701, Turns: 35\n",
            "Winrate: 0.008333333333333333\n",
            "memory size: 2048\n",
            "[11:51:46] Episode: 121/10000, Wins: 1 - 120, Scores: 38 - 85, Epsilon: 0.0, Total reward: -1.2893, Turns: 27\n",
            "Winrate: 0.008264462809917356\n",
            "memory size: 2048\n",
            "[11:51:52] Episode: 122/10000, Wins: 1 - 121, Scores: 26 - 86, Epsilon: 0.0, Total reward: -1.3304, Turns: 18\n",
            "Winrate: 0.00819672131147541\n",
            "memory size: 2048\n",
            "[11:52:00] Episode: 123/10000, Wins: 1 - 122, Scores: 37 - 88, Epsilon: 0.0, Total reward: -1.3188999999999997, Turns: 17\n",
            "Winrate: 0.008130081300813009\n",
            "memory size: 2048\n",
            "[11:52:08] Episode: 124/10000, Wins: 1 - 123, Scores: 12 - 90, Epsilon: 0.0, Total reward: -1.4700000000000002, Turns: 18\n",
            "Winrate: 0.008064516129032258\n",
            "memory size: 2048\n",
            "[11:52:15] Episode: 125/10000, Wins: 1 - 124, Scores: 13 - 84, Epsilon: 0.0, Total reward: -1.5575, Turns: 16\n",
            "Winrate: 0.008\n",
            "memory size: 2048\n",
            "Target update counter: 3946 / 10000.0\n",
            "Saving...\n",
            "Model has saved at 125\n",
            "Total: 12.68 GB\n",
            "Available: 8.55 GB\n",
            "Used: 3.84 GB\n",
            "Percentage usage: 32.6%\n",
            "[11:52:26] Episode: 126/10000, Wins: 1 - 125, Scores: 28 - 84, Epsilon: 0.0, Total reward: -1.2997999999999998, Turns: 18\n",
            "Winrate: 0.007936507936507936\n",
            "memory size: 2048\n",
            "[11:52:33] Episode: 127/10000, Wins: 1 - 126, Scores: 40 - 86, Epsilon: 0.0, Total reward: -1.1796000000000002, Turns: 16\n",
            "Winrate: 0.007874015748031496\n",
            "memory size: 2048\n",
            "[11:52:41] Episode: 128/10000, Wins: 1 - 127, Scores: 31 - 83, Epsilon: 0.0, Total reward: -1.4694, Turns: 22\n",
            "Winrate: 0.0078125\n",
            "memory size: 2048\n",
            "[11:52:47] Episode: 129/10000, Wins: 1 - 128, Scores: 45 - 84, Epsilon: 0.0, Total reward: -1.2703, Turns: 20\n",
            "Winrate: 0.007751937984496124\n",
            "memory size: 2048\n",
            "[11:52:56] Episode: 130/10000, Wins: 1 - 129, Scores: 22 - 84, Epsilon: 0.0, Total reward: -1.4678, Turns: 23\n",
            "Winrate: 0.007692307692307693\n",
            "memory size: 2048\n",
            "[11:53:01] Episode: 131/10000, Wins: 1 - 130, Scores: 13 - 82, Epsilon: 0.0, Total reward: -1.6095, Turns: 8\n",
            "Winrate: 0.007633587786259542\n",
            "memory size: 2048\n",
            "[11:53:08] Episode: 132/10000, Wins: 1 - 131, Scores: 10 - 90, Epsilon: 0.0, Total reward: -1.4913999999999998, Turns: 10\n",
            "Winrate: 0.007575757575757576\n",
            "memory size: 2048\n",
            "[11:53:15] Episode: 133/10000, Wins: 1 - 132, Scores: 24 - 94, Epsilon: 0.0, Total reward: -1.3492000000000002, Turns: 20\n",
            "Winrate: 0.007518796992481203\n",
            "memory size: 2048\n",
            "[11:53:23] Episode: 134/10000, Wins: 1 - 133, Scores: 22 - 82, Epsilon: 0.0, Total reward: -1.3568, Turns: 14\n",
            "Winrate: 0.007462686567164179\n",
            "memory size: 2048\n",
            "[11:53:29] Episode: 135/10000, Wins: 1 - 134, Scores: 10 - 82, Epsilon: 0.0, Total reward: -1.4907999999999997, Turns: 8\n",
            "Winrate: 0.007407407407407408\n",
            "memory size: 2048\n",
            "[11:53:39] Episode: 136/10000, Wins: 1 - 135, Scores: 31 - 82, Epsilon: 0.0, Total reward: -1.3693, Turns: 41\n",
            "Winrate: 0.007352941176470588\n",
            "memory size: 2048\n",
            "[11:53:47] Episode: 137/10000, Wins: 1 - 136, Scores: 19 - 83, Epsilon: 0.0, Total reward: -1.2297999999999998, Turns: 27\n",
            "Winrate: 0.0072992700729927005\n",
            "memory size: 2048\n",
            "[11:53:53] Episode: 138/10000, Wins: 1 - 137, Scores: 10 - 90, Epsilon: 0.0, Total reward: -1.4913999999999996, Turns: 11\n",
            "Winrate: 0.007246376811594203\n",
            "memory size: 2048\n",
            "[11:54:00] Episode: 139/10000, Wins: 1 - 138, Scores: 10 - 82, Epsilon: 0.0, Total reward: -1.4924000000000002, Turns: 12\n",
            "Winrate: 0.007194244604316547\n",
            "memory size: 2048\n",
            "[11:54:06] Episode: 140/10000, Wins: 1 - 139, Scores: 13 - 82, Epsilon: 0.0, Total reward: -1.6111, Turns: 12\n",
            "Winrate: 0.007142857142857143\n",
            "memory size: 2048\n",
            "[11:54:13] Episode: 141/10000, Wins: 1 - 140, Scores: 10 - 86, Epsilon: 0.0, Total reward: -1.4878, Turns: 18\n",
            "Winrate: 0.0070921985815602835\n",
            "memory size: 2048\n",
            "[11:54:21] Episode: 142/10000, Wins: 1 - 141, Scores: 14 - 86, Epsilon: 0.0, Total reward: -1.4501999999999997, Turns: 18\n",
            "Winrate: 0.007042253521126761\n",
            "memory size: 2048\n",
            "[11:54:29] Episode: 143/10000, Wins: 1 - 142, Scores: 21 - 83, Epsilon: 0.0, Total reward: -1.2697999999999998, Turns: 23\n",
            "Winrate: 0.006993006993006993\n",
            "memory size: 2048\n",
            "[11:54:35] Episode: 144/10000, Wins: 1 - 143, Scores: 14 - 86, Epsilon: 0.0, Total reward: -1.4322, Turns: 17\n",
            "Winrate: 0.006944444444444444\n",
            "memory size: 2048\n"
          ]
        }
      ],
      "source": [
        "#conv2d dqn vs minimax\n",
        "#!pip show tensorflow\n",
        "#!pip install --upgrade tensorflow\n",
        "#!pip install pydot\n",
        "#!pip install graphviz\n",
        "import math, random, inspect, time, datetime, random, os, psutil, gc\n",
        "import numpy as np\n",
        "from timeit import default_timer as timer\n",
        "from copy import deepcopy\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "from datetime import timedelta\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Concatenate, Multiply, Dropout, Conv1D, Conv2D, Flatten, MaxPooling2D, ZeroPadding2D, BatchNormalization, Activation, AveragePooling2D, LSTM\n",
        "from keras.utils import plot_model\n",
        "from collections import deque\n",
        "\n",
        "class Player:\n",
        "    def __init__(self, name=\"Unknown player\"):\n",
        "        self.score = 0\n",
        "        self.name = name\n",
        "\n",
        "class Human(Player):\n",
        "    def __init__(self, name):\n",
        "        Player.__init__(self, name)\n",
        "\n",
        "\n",
        "class AI(Player):\n",
        "    def __init__(self, name):\n",
        "        Player.__init__(self, name)\n",
        "\n",
        "    def getMove(self):\n",
        "        pass\n",
        "\n",
        "class MinimaxAI(AI):\n",
        "    def __init__(self, name):\n",
        "        AI.__init__(self, name)\n",
        "\n",
        "    def getMove(self, board, depth):\n",
        "        bestmove = None\n",
        "        besteval = -math.inf\n",
        "\n",
        "        bestmoves = []\n",
        "        possibleMoves = board.getPossibleMoves()\n",
        "        start = datetime.now()\n",
        "        for i, move in enumerate(possibleMoves):\n",
        "            board.makeMove(move[0], move[1])\n",
        "            eval = Board.minimax(board, self, depth, -float(\"inf\"), float(\"inf\"), True)\n",
        "            board.undoMove()\n",
        "            if eval > besteval:\n",
        "                besteval = eval\n",
        "                bestmoves = [move]\n",
        "            elif abs(eval - besteval) <= 0:\n",
        "                bestmoves.append(move)\n",
        "        move = random.choice(bestmoves)\n",
        "        return move\n",
        "\n",
        "class RandomAI(AI):\n",
        "    def __init__(self, name):\n",
        "        AI.__init__(self, name)\n",
        "\n",
        "    def getMove(self, board):\n",
        "        return random.choice(board.possibleMoves)\n",
        "\n",
        "class DQN(AI):\n",
        "    def __init__(self, name):\n",
        "        AI.__init__(self, name)\n",
        "        self.action_size = 9\n",
        "        self.memory = deque(maxlen=2048)  # хранение опыта\n",
        "        self.gamma = 0.95 # коэффициент дисконтирования\n",
        "        self.epsilon = 0.0 # исследование против эксплуатации\n",
        "        self.epsilon_min = 0.1\n",
        "        self.epsilon_decay = 0.9995\n",
        "        self.learning_rate = 0.0001\n",
        "        self.model = self._build_model()\n",
        "        self.target_model = self._build_model()\n",
        "        self.update_target_model()\n",
        "        self.target_update_counter = 0\n",
        "        self.update_frequency = 1e4\n",
        "\n",
        "    def _build_model(self):\n",
        "        # Входные слои для различных типов данных\n",
        "        input_board = Input(shape=(2, 9, 1), name=\"board\")\n",
        "        input_tuzdyk = Input(shape=(2,), name=\"tuzdyk\")\n",
        "        input_kazan = Input(shape=(2,), name=\"kazan\")\n",
        "\n",
        "        #zeropad = ZeroPadding2D(padding = 2)(input_board)\n",
        "        board_features = Conv2D(81, (1, 1), padding = \"same\", activation='linear')(input_board)\n",
        "        board_features = Conv2D(162, (2, 2), padding = \"same\", activation='linear')(board_features)\n",
        "        board_features = Conv2D(162, (2, 2), padding = \"same\", activation='linear')(board_features)\n",
        "        #board_features = AveragePooling2D()(activation)\n",
        "        flatten = Flatten()(board_features)\n",
        "\n",
        "        merged = Concatenate()([flatten, input_tuzdyk, input_kazan])\n",
        "\n",
        "        # Четвертый блок слоев, работающий над объединенными данными\n",
        "        merged_features = Dense(162, activation=\"linear\")(merged)\n",
        "        merged_features = Dropout(0.1)(merged_features)\n",
        "        merged_features = Dense(162, activation=\"linear\")(merged_features)\n",
        "        merged_features = Dense(162, activation=\"linear\")(merged_features)\n",
        "        merged_features = Dropout(0.1)(merged_features)\n",
        "\n",
        "        # Пятый блок слоев, отвечающий за выход\n",
        "        output_layer = Dense(9, activation=\"linear\")(merged_features)\n",
        "\n",
        "        #merged = Concatenate()([output_layer, input_kazan])\n",
        "\n",
        "        # Седьмой блок слоев, дополнительная обработка перед финальным выходом\n",
        "        final_features = Dense(81, activation=\"linear\")(output_layer)\n",
        "        final_features = Dense(81, activation=\"linear\")(final_features)\n",
        "        final_features = Dense(18, activation=\"linear\")(final_features)\n",
        "\n",
        "\n",
        "        # Финальный выходной слой\n",
        "        output = Dense(self.action_size, activation=\"linear\")(final_features)\n",
        "\n",
        "        # Составление модели\n",
        "        model = Model(inputs=[input_board, input_kazan, input_tuzdyk], outputs=output)\n",
        "        model.compile(\n",
        "            loss=\"mse\", optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
        "        )\n",
        "        model.summary()\n",
        "        plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n",
        "        return model\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    def getState(self, board):\n",
        "        tuzdyk1 = -1\n",
        "        tuzdyk2 = -1\n",
        "        if board.tuzdyk1 != None:\n",
        "            tuzdyk1 = board.tuzdyk1[0]\n",
        "        if board.tuzdyk2 != None:\n",
        "            tuzdyk2 = board.tuzdyk2[0]\n",
        "        input_board = [board.board[0], board.board[1]]\n",
        "        kazan = np.array([board.player1.score, board.player2.score])\n",
        "        tuzdyk = np.array([tuzdyk1, tuzdyk2])\n",
        "        if self != board.player1:\n",
        "            kazan = np.array([board.player2.score, board.player1.score])\n",
        "            input_board = [board.board[1], board.board[0]]\n",
        "            tuzdyk = np.array([tuzdyk2, tuzdyk1])\n",
        "\n",
        "        input_board = np.array(input_board).reshape(1, 2, 9)\n",
        "        #print(input_board)\n",
        "        kazan = kazan.reshape(1, 2)\n",
        "        tuzdyk = tuzdyk.reshape(1, 2)\n",
        "        input_board = tf.convert_to_tensor(input_board, dtype=tf.int32)\n",
        "        kazan = tf.convert_to_tensor(kazan, dtype=tf.int32)\n",
        "        tuzdyk = tf.convert_to_tensor(tuzdyk, dtype=tf.int32)\n",
        "        return [input_board, kazan, tuzdyk]\n",
        "\n",
        "    def remember(self, state, action, reward, next_state):\n",
        "        self.memory.append((state, action, reward, next_state))\n",
        "\n",
        "    def act(self, state, board):\n",
        "        if np.random.rand() <= self.epsilon:# or len(board.moveHistory) <= 2:\n",
        "            move = random.choice(board.possibleMoves)\n",
        "            return move # случайное действие\n",
        "        act_values = self.model.predict(state, verbose=0)\n",
        "        #print(act_values)\n",
        "        possible_moves = board.possibleMoves\n",
        "        y = 0 if self == board.player1 else 1\n",
        "        for i in range(self.action_size):\n",
        "            if (i, y) not in possible_moves:\n",
        "                act_values[0][i] = -math.inf\n",
        "        return (np.argmax(act_values[0]), 0 if self == board.player1 else 1)  # лучшее действие\n",
        "\n",
        "    def replay(self, batch_size=32):\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        for state, action, reward, next_state in minibatch:\n",
        "            target = reward\n",
        "\n",
        "            if next_state is not None:\n",
        "                target = reward + self.gamma * np.max(self.target_model.predict(next_state, verbose = 0)[0])\n",
        "            target_f = self.model.predict(state, verbose = 0)\n",
        "            target_f[0][action] = target\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                q_values = self.model(state)\n",
        "                loss = tf.reduce_mean(tf.square(target_f - q_values))\n",
        "            grads = tape.gradient(loss, self.model.trainable_variables)\n",
        "            self.model.optimizer.apply_gradients(zip(grads, self.model.trainable_variables), verbose = 0)\n",
        "            self.target_update_counter += 1\n",
        "            if self.target_update_counter >= self.update_frequency:\n",
        "                print(\"Target network has been updated\")\n",
        "                self.update_target_model()\n",
        "                self.target_update_counter = 0\n",
        "\n",
        "        # Уменьшить epsilon\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model = tf.keras.models.load_model(name)\n",
        "        self.update_target_model()\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save(name, overwrite = True)\n",
        "\n",
        "    def load_weights(self, name):\n",
        "        self.model.load_weights(name)\n",
        "        self.update_target_model()\n",
        "\n",
        "    def save_weights(self, name):\n",
        "        self.model.save_weights(name, overwrite = True)\n",
        "\n",
        "\n",
        "class Board:\n",
        "    def __init__(self, player1, player2):\n",
        "        self.board = [[9,9,9,9,9,9,9,9,9],\n",
        "                      [9,9,9,9,9,9,9,9,9]]\n",
        "        self.player1 = player1\n",
        "        self.player2 = player2\n",
        "        self.turnOwner = player1\n",
        "        self.tuzdyk1 = None\n",
        "        self.tuzdyk2 = None\n",
        "        self.moveHistory = []\n",
        "        self.semiTurns = 1\n",
        "        self.fullTurns = 1\n",
        "        self.lastMove = None\n",
        "        self.possibleMoves = self.getPossibleMoves()\n",
        "\n",
        "    def setTKFEN(self, fen):\n",
        "        self.reset()\n",
        "        fen_elements = fen.split()\n",
        "        state = fen_elements[0].split(\"-\")\n",
        "        for i, cell in enumerate(state):\n",
        "            x = i % 9\n",
        "            y = i // 9\n",
        "            if cell == \"X\":\n",
        "                if y == 0:\n",
        "                    self.tuzdyk2 = (x, y)\n",
        "                else:\n",
        "                    self.tuzdyk1 = (x, y)\n",
        "                self.board[y][x] = 0\n",
        "            else:\n",
        "                self.board[y][x] = int(cell)\n",
        "        current_turn = fen_elements[1]\n",
        "        if current_turn == \"w\":\n",
        "            self.turnOwner = self.player1\n",
        "        else:\n",
        "            self.turnOwner = self.player2\n",
        "        self.player1.score = int(fen_elements[2])\n",
        "        self.player2.score = int(fen_elements[3])\n",
        "        self.semiTurns = int(fen_elements[4])\n",
        "        self.fullTurns = int(fen_elements[5])\n",
        "        self.possibleMoves = self.getPossibleMoves()\n",
        "\n",
        "    def getTKFEN(self):\n",
        "        board = \"\"\n",
        "        for y, row in enumerate(self.board):\n",
        "            for x, item in enumerate(row):\n",
        "                if self.tuzdyk1 == (x, y) or self.tuzdyk2 == (x, y):\n",
        "                    board += 'X'\n",
        "                else:\n",
        "                    board += str(item)\n",
        "                if x != len(row) - 1:\n",
        "                    board += \"-\"\n",
        "            if y != len(self.board) - 1:\n",
        "                board += \"-\"\n",
        "        turnOwner = \"w\"\n",
        "        if self.turnOwner == self.player2:\n",
        "            turnOwner = \"b\"\n",
        "        return f\"{board} {turnOwner} {self.player1.score} {self.player2.score} {self.semiTurns} {self.fullTurns}\"\n",
        "\n",
        "    def getState(self):\n",
        "        return (\n",
        "            str(self.board),\n",
        "            self.turnOwner == self.player1,\n",
        "            (self.tuzdyk1, self.tuzdyk2),\n",
        "            (self.player1.score, self.player2.score),\n",
        "        )\n",
        "\n",
        "    def reset(self):\n",
        "        self.board = [[9,9,9,9,9,9,9,9,9],\n",
        "                      [9,9,9,9,9,9,9,9,9]]\n",
        "        self.moveHistory.clear()\n",
        "        self.player1.score = self.player2.score = 0\n",
        "        self.turnOwner = self.player1\n",
        "        self.tuzdyk1 = self.tuzdyk2 = None\n",
        "        self.semiTurns = self.fullTurns = 1\n",
        "        self.possibleMoves = self.getPossibleMoves()\n",
        "\n",
        "    def showBoard(self):\n",
        "        player1Desk = \"\".join(\n",
        "            f\"{self.board[0][i] if self.tuzdyk2 != (i, 0) else 'X'} \" for i in range(9)\n",
        "        )\n",
        "        player2Desk = \"\".join(\n",
        "            f\"{self.board[1][i] if self.tuzdyk1 != (i, 1) else 'X'} \"\n",
        "            for i in reversed(range(9))\n",
        "        )\n",
        "        print(\"__________________________\")\n",
        "        print(f\"Turn: {self.semiTurns}\\n Makes turn: {self.turnOwner.name}\")\n",
        "        print(f\"{self.player2.name} Score: {self.player2.score}\")\n",
        "        print(\"\".join([f\"{i} \" for i in reversed(range(9))]))\n",
        "        print()\n",
        "        print(player2Desk)\n",
        "        print(player1Desk)\n",
        "        print()\n",
        "        print(\"\".join([f\"{i} \" for i in range(9)]))\n",
        "        print(f\"{self.player1.name} Score: {self.player1.score}\")\n",
        "        print(\"__________________________\")\n",
        "\n",
        "    def predict(self, x, y):\n",
        "        stones = self.getStoneCountAtCell(x, y)\n",
        "        if stones == 0:\n",
        "            return (x, y)\n",
        "        elif stones == 1:\n",
        "            if x == 8:\n",
        "                return (0, 0 if y == 1 else 1)\n",
        "            else:\n",
        "                return (x + 1, y)\n",
        "        else:\n",
        "            stones -= 1\n",
        "        maxstones = stones\n",
        "        while stones > 0:\n",
        "            stones = stones - (8 - x)\n",
        "            x += maxstones - stones\n",
        "            maxstones = stones\n",
        "            if stones <= 0 and x + maxstones <= 8:\n",
        "                return (x + maxstones, y)\n",
        "            if x >= 8:\n",
        "                x = -1\n",
        "                y = 0 if y == 1 else 1\n",
        "        return (x, y)\n",
        "\n",
        "    def evaluate(self, player):\n",
        "        win_factor = 0\n",
        "        score_factor = player.score\n",
        "        tuzdyk_factor = 0\n",
        "        possible_moves_factor = self.getPossibleMovesForPlayer(player)\n",
        "        desk_stones_factor = self.getPlayerDeskStones(player)\n",
        "\n",
        "        if player == self.player1:\n",
        "            winner = self.checkWinner()\n",
        "            score_factor -= self.player2.score\n",
        "            desk_stones_factor -= self.getPlayerDeskStones(self.player2)\n",
        "            possible_moves_factor -= self.getPossibleMovesForPlayer(self.player2)\n",
        "            if winner == self.player1:\n",
        "                win_factor = 162 - self.player1.score\n",
        "            elif winner == self.player2:\n",
        "                win_factor = self.player2.score - 162\n",
        "            if self.tuzdyk1 is not None:\n",
        "                tuzdyk_factor += 4 - abs(self.tuzdyk1[0] - 4) * 5\n",
        "            if self.tuzdyk2 is not None:\n",
        "                tuzdyk_factor -= 4 - abs(self.tuzdyk2[0] - 4) * 5\n",
        "        elif player == self.player2:\n",
        "            winner = self.checkWinner()\n",
        "            score_factor -= self.player1.score\n",
        "            desk_stones_factor -= self.getPlayerDeskStones(self.player1)\n",
        "            possible_moves_factor -= self.getPossibleMovesForPlayer(self.player1)\n",
        "            if winner == self.player2:\n",
        "                win_factor = 162 - self.player2.score\n",
        "            elif winner == self.player1:\n",
        "                win_factor = self.player1.score - 162\n",
        "            if self.tuzdyk2 is not None:\n",
        "                tuzdyk_factor += 4 - abs(self.tuzdyk2[0] - 4) * 5\n",
        "            if self.tuzdyk1 is not None:\n",
        "                tuzdyk_factor -= 4 - abs(self.tuzdyk1[0] - 4) * 5\n",
        "\n",
        "        vulnerableCells, vulnerableStones = self.getVulnerables(player)\n",
        "        attackableCells, attackableStones = self.getAttackables(player)\n",
        "        factors = [\n",
        "            score_factor,\n",
        "            desk_stones_factor * 0.01,\n",
        "            -vulnerableCells,\n",
        "            -vulnerableStones,\n",
        "            tuzdyk_factor,\n",
        "            attackableCells,\n",
        "            attackableStones,\n",
        "            win_factor,\n",
        "            possible_moves_factor\n",
        "        ]\n",
        "\n",
        "        eval = sum(factors)\n",
        "        return eval\n",
        "\n",
        "    def minimax(board, player, depth, alpha, beta, maximizing_player):\n",
        "        if depth == 0 or board.checkWinner() is not None:\n",
        "            return board.evaluate(player)\n",
        "        if maximizing_player:\n",
        "            max_eval = float('-inf')\n",
        "            for move in board.possibleMoves:\n",
        "                board.makeMove(move[0], move[1])  # Реализуйте функцию, делающую ход в игре\n",
        "                eval = Board.minimax(board, player, depth - 1, alpha, beta, False)\n",
        "                board.undoMove()\n",
        "                max_eval = max(max_eval, eval)\n",
        "                alpha = max(alpha, eval)\n",
        "                if beta <= alpha:\n",
        "                    break\n",
        "            return max_eval\n",
        "        else:\n",
        "            min_eval = float('inf')\n",
        "            for move in board.possibleMoves:\n",
        "                board.makeMove(move[0], move[1])  # Реализуйте функцию, делающую ход в игре\n",
        "                eval = Board.minimax(board, player, depth - 1, alpha, beta, True)\n",
        "                board.undoMove()\n",
        "                min_eval = min(min_eval, eval)\n",
        "                beta = min(beta, eval)\n",
        "                if beta <= alpha:\n",
        "                    break\n",
        "            return min_eval\n",
        "\n",
        "\n",
        "    def getPossibleMovesForPlayer(self, player):\n",
        "        totalMoves = 0\n",
        "        if self.turnOwner != player:\n",
        "            if player == self.player1:\n",
        "                for x in range(9):\n",
        "                    if self.getStoneCountAtCell(x, 0) != 0:\n",
        "                        totalMoves += 1\n",
        "            else:\n",
        "                for x in range(9):\n",
        "                    if self.getStoneCountAtCell(x, 1) != 0:\n",
        "                        totalMoves += 1\n",
        "        else:\n",
        "            totalMoves = len(self.possibleMoves)\n",
        "        return totalMoves\n",
        "\n",
        "    def getVulnerables(self, player):\n",
        "        totalCells = 0\n",
        "        loss = float(\"-inf\")\n",
        "        if self.turnOwner == player:\n",
        "            loss = float(\"-inf\")\n",
        "            myMoves = self.possibleMoves\n",
        "            for myMove in myMoves:\n",
        "              self.makeMove(myMove[0], myMove[1])\n",
        "              totalCells = 0\n",
        "              oppMoves = self.possibleMoves\n",
        "              for oppMove in oppMoves:\n",
        "                attackedPosition = self.predict(oppMove[0], oppMove[1])\n",
        "                attackedStones = self.getStoneCountAtCell(attackedPosition[0], attackedPosition[1])\n",
        "                stonesInArm = self.getStoneCountAtCell(oppMove[0], oppMove[1]) - 1\n",
        "                if oppMove[1] != attackedPosition[1] and attackedStones % 2 != 0:\n",
        "                    attackedStones = attackedStones + stonesInArm // 18 + 1\n",
        "                    if loss == None or attackedStones > loss:\n",
        "                        loss = attackedStones\n",
        "                    totalCells += 1\n",
        "              self.undoMove()\n",
        "        else:\n",
        "          totalCells = 0\n",
        "          oppMoves = self.possibleMoves\n",
        "          for oppMove in oppMoves:\n",
        "            attackedPosition = self.predict(oppMove[0], oppMove[1])\n",
        "            attackedStones = self.getStoneCountAtCell(attackedPosition[0], attackedPosition[1])\n",
        "            stonesInArm = self.getStoneCountAtCell(oppMove[0], oppMove[1]) - 1\n",
        "            if oppMove[1] != attackedPosition[1] and attackedStones % 2 != 0:\n",
        "                attackedStones = attackedStones + stonesInArm // 18 + 1\n",
        "                if loss == None or attackedStones > loss:\n",
        "                    loss = attackedStones\n",
        "                totalCells += 1\n",
        "        if loss == float(\"-inf\"):\n",
        "            loss = 0\n",
        "        return totalCells, loss\n",
        "\n",
        "    def getAttackables(self, player):\n",
        "        captured = None\n",
        "        totalCells = 0\n",
        "        if player == self.turnOwner:\n",
        "            for move in self.possibleMoves:\n",
        "                attackedPosition = self.predict(move[0], move[1])\n",
        "                attackedStones = self.getStoneCountAtCell(attackedPosition[0], attackedPosition[1])\n",
        "                stonesInArm = self.getStoneCountAtCell(move[0], move[1]) - 1\n",
        "                if move[1] != attackedPosition[1] and attackedStones % 2 != 0:\n",
        "                    attackedStones = attackedStones + stonesInArm // 18 + 1\n",
        "                    if captured == None or attackedStones > captured:\n",
        "                        captured = attackedStones\n",
        "                    totalCells += 1\n",
        "            if captured == None:\n",
        "                captured = 0\n",
        "        else:\n",
        "            captured = float(\"-inf\")\n",
        "            for move in self.possibleMoves:\n",
        "                capturedStones = None\n",
        "                cells = 0\n",
        "                self.makeMove(move[0], move[1])\n",
        "                for move in self.possibleMoves:\n",
        "                    attackedPosition = self.predict(move[0], move[1])\n",
        "                    attackedStones = self.getStoneCountAtCell(attackedPosition[0], attackedPosition[1])\n",
        "                    stonesInArm = self.getStoneCountAtCell(move[0], move[1]) - 1\n",
        "                    if move[1] != attackedPosition[1] and attackedStones % 2 != 0:\n",
        "                        attackedStones = attackedStones + stonesInArm // 18 + 1\n",
        "                        if capturedStones == None or attackedStones > capturedStones:\n",
        "                            capturedStones = attackedStones\n",
        "                        cells += 1\n",
        "                self.undoMove()\n",
        "                if capturedStones is None:\n",
        "                    capturedStones = 0\n",
        "                if capturedStones > captured:\n",
        "                    captured = capturedStones\n",
        "                    totalCells = cells\n",
        "            if captured == float(\"-inf\") or captured is None:\n",
        "                captured = 0\n",
        "        return totalCells, captured\n",
        "\n",
        "    def getPlayerDeskStones(self, player):\n",
        "        result = 0\n",
        "        if player == self.player1:\n",
        "            for i in range(9):\n",
        "                result += self.getStoneCountAtCell(i, 0)\n",
        "        else:\n",
        "            for i in range(9):\n",
        "                result += self.getStoneCountAtCell(i, 1)\n",
        "        return result\n",
        "\n",
        "    def checkWinner(self):\n",
        "        if self.player1.score >= 82 or (\n",
        "            self.player2.score < 82 and self.isDeskEmpty(self.player2)\n",
        "        ):\n",
        "            return self.player1\n",
        "        elif self.player2.score >= 82 or (\n",
        "            self.player1.score < 82 and self.isDeskEmpty(self.player1)\n",
        "        ):\n",
        "            return self.player2\n",
        "        return None\n",
        "\n",
        "    def isDeskEmpty(self, player):\n",
        "        index = 0\n",
        "        if player == self.player2:\n",
        "            index = 1\n",
        "        for i in range(9):\n",
        "            if self.board[index][i] > 0:\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def isValidMove(self, x, y):\n",
        "        return (\n",
        "            (self.turnOwner == self.player1 and y == 0)\n",
        "            or (self.turnOwner == self.player2 and y == 1)\n",
        "        ) and self.isCellEmpty(x, y) == False\n",
        "\n",
        "    def isCellEmpty(self, x, y):\n",
        "        return self.board[y][x] == 0\n",
        "\n",
        "    def getStoneCountAtCell(self, x, y):\n",
        "        return self.board[y][x]\n",
        "\n",
        "    def getPossibleMoves(self):\n",
        "        possibleMoves = []\n",
        "        if self.checkWinner() is not None:\n",
        "            return possibleMoves\n",
        "        if self.turnOwner == self.player1:\n",
        "            for x in range(9):\n",
        "                if not self.isCellEmpty(x, 0):\n",
        "                    possibleMoves.append((x, 0))\n",
        "        else:\n",
        "            for x in range(9):\n",
        "                if not self.isCellEmpty(x, 1):\n",
        "                    possibleMoves.append((x, 1))\n",
        "        return possibleMoves\n",
        "\n",
        "    def switchPlayer(self):\n",
        "        if self.turnOwner == self.player1:\n",
        "            self.turnOwner = self.player2\n",
        "        else:\n",
        "            self.fullTurns += 1\n",
        "            self.turnOwner = self.player1\n",
        "\n",
        "    def makeMove(self, x, y):\n",
        "        stonesInArm = 0\n",
        "        if not self.isValidMove(x, y) or self.checkWinner() != None:\n",
        "            return False\n",
        "        self.lastMove = \"\"\n",
        "        self.lastMove = f\"{x + 1}\"\n",
        "        self.semiTurns += 1\n",
        "        self.moveHistory.append((\n",
        "                Board.copyBoard(self.board),\n",
        "                self.turnOwner,\n",
        "                self.tuzdyk1,\n",
        "                self.tuzdyk2,\n",
        "                self.player1.score,\n",
        "                self.player2.score))\n",
        "        if self.getStoneCountAtCell(x, y) > 1:\n",
        "            stonesInArm = self.getStoneCountAtCell(x, y) - 1\n",
        "            self.board[y][x] = 1\n",
        "        else:\n",
        "            stonesInArm = 1\n",
        "            self.board[y][x] = 0\n",
        "        position = (x, y)\n",
        "        for _ in range(stonesInArm):\n",
        "            position = ((position[0] + 1) % 9, 1 - position[1] if position[0] == 8 else position[1])\n",
        "\n",
        "            if position == self.tuzdyk1:\n",
        "                self.player1.score += 1\n",
        "            elif position == self.tuzdyk2:\n",
        "                self.player2.score += 1\n",
        "            else:\n",
        "                self.board[position[1]][position[0]] += 1\n",
        "\n",
        "        x, y = position\n",
        "        if self.board[y][x] == 3:  # make tuzdyk\n",
        "            if (self.turnOwner == self.player1 and x != 8 and y == 1 and self.tuzdyk1 == None):\n",
        "                if (self.tuzdyk2 != None and self.tuzdyk2[0] != x) or self.tuzdyk2 == None:\n",
        "                    self.tuzdyk1 = (x, y)\n",
        "                    self.board[y][x] = 0  # player 1\n",
        "                    self.player1.score += 3\n",
        "            elif (self.turnOwner == self.player2 and x != 8 and y == 0 and self.tuzdyk2 == None):\n",
        "                if (self.tuzdyk1 != None and self.tuzdyk1[0] != x) or self.tuzdyk1 == None:\n",
        "                    self.tuzdyk2 = (x, y)\n",
        "                    self.board[y][x] = 0  # player 2\n",
        "                    self.player2.score += 3\n",
        "        if self.board[y][x] % 2 == 0:  # grab stones\n",
        "            if (self.turnOwner == self.player1 and y == 1) or (self.turnOwner == self.player2 and y == 0):\n",
        "                self.turnOwner.score += self.board[y][x]\n",
        "                self.board[y][x] = 0\n",
        "        self.switchPlayer()\n",
        "        self.lastMove += f\"{x+1}\"\n",
        "        self.possibleMoves = self.getPossibleMoves()\n",
        "        return True\n",
        "\n",
        "    def undoMove(self):\n",
        "        if len(self.moveHistory) == 0:\n",
        "            return False\n",
        "        lastMove = self.moveHistory.pop()\n",
        "        self.board = lastMove[0]\n",
        "        self.turnOwner = lastMove[1]\n",
        "        self.tuzdyk1 = lastMove[2]\n",
        "        self.tuzdyk2 = lastMove[3]\n",
        "        self.player1.score = lastMove[4]\n",
        "        self.player2.score = lastMove[5]\n",
        "        self.semiTurns -= 1\n",
        "        self.possibleMoves = self.getPossibleMoves()\n",
        "        if self.turnOwner == self.player2:\n",
        "            self.fullTurns -= 1\n",
        "        return True\n",
        "\n",
        "    def copyBoard(boardArray):\n",
        "        return [row[:] for row in boardArray]\n",
        "\n",
        "player1 = DQN(\"DQN-1\")\n",
        "player2 = MinimaxAI(\"Minimax\")\n",
        "#player1.load(\"DQN_convmodelD2\")\n",
        "#player1.load_weights(\"/content/drive/MyDrive/Togyzkumalak/DQN_convmodel\")\n",
        "#player1.load(\"/kaggle/input/togyzkumalakdb/MODELMCTS\")\n",
        "board = Board(player1, player2)\n",
        "\n",
        "# Параметры обучения\n",
        "n_episodes = 10000  # количество эпизодов\n",
        "batch_size = 32  # размер пакета для обучения\n",
        "white_wins = 0\n",
        "black_wins = 0\n",
        "rewards = []\n",
        "last_checking_rewards = []\n",
        "last_rewards_len = 16\n",
        "start = timer()\n",
        "for episode in range(n_episodes):\n",
        "    state = None\n",
        "    done = False\n",
        "    myMove = None\n",
        "    oppMove = None\n",
        "    reward = 0\n",
        "    total_reward = 0\n",
        "    game_rewards = []\n",
        "    if episode % 25 == 0:\n",
        "        print(\"Target update counter:\", player1.target_update_counter, \"/\", player1.update_frequency)\n",
        "        print(\"Saving...\")\n",
        "        player1.save(\"/content/drive/MyDrive/Togyzkumalak/DQN_convmodel\")\n",
        "        player1.save_weights(\"/content/drive/MyDrive/Togyzkumalak/DQN_convmodel\")\n",
        "        print(f\"Model has saved at {episode}\")\n",
        "        ram_info = psutil.virtual_memory()\n",
        "        print(f\"Total: {ram_info.total / 1024 / 1024 / 1024:.2f} GB\")\n",
        "        print(f\"Available: {ram_info.available / 1024 / 1024 / 1024:.2f} GB\")\n",
        "        print(f\"Used: {ram_info.used / 1024 / 1024 / 1024:.2f} GB\")\n",
        "        print(f\"Percentage usage: {ram_info.percent}%\")\n",
        "        gc.collect()\n",
        "        tf.keras.backend.clear_session()\n",
        "    while done == False:\n",
        "        #board.showBoard()\n",
        "        #print(board.evaluate(player1), board.evaluate(player2))\n",
        "        if board.checkWinner() is not None:\n",
        "            if board.checkWinner() == board.player1:\n",
        "                white_wins += 1\n",
        "            elif board.checkWinner() == board.player2:\n",
        "                black_wins += 1\n",
        "            if len(player1.memory) // 2 < batch_size:\n",
        "                player1.replay(len(player1.memory) // 2)\n",
        "            else:\n",
        "                player1.replay(batch_size)\n",
        "            curr_time = time.strftime(\"%H:%M:%S\", time.localtime())\n",
        "            print(f\"[{curr_time}] Episode: {episode+1}/{n_episodes}, Wins: {white_wins} - {black_wins}, Scores: {board.player1.score} - {board.player2.score}, Epsilon: {player1.epsilon}, Total reward: {total_reward}, Turns: {board.fullTurns}\\nWinrate: {white_wins/(episode + 1)}\")\n",
        "            board.reset()\n",
        "            print(\"memory size:\", len(player1.memory))\n",
        "            break\n",
        "        if board.turnOwner == player1:\n",
        "            state = player1.getState(board)\n",
        "            myMove = player1.act(state, board)\n",
        "            reward = Board.minimax(board, player1, 2, float(\"-inf\"), float(\"inf\"), True)\n",
        "            board.makeMove(myMove[0], myMove[1])\n",
        "        else:\n",
        "            oppMove = player2.getMove(board, 1)\n",
        "            board.makeMove(oppMove[0], oppMove[1])\n",
        "            if myMove != None:\n",
        "                reward = (Board.minimax(board, player1, 2, float(\"-inf\"), float(\"inf\"), True) - reward) * 0.01\n",
        "                next_state = player1.getState(board)\n",
        "                #print(reward)\n",
        "                total_reward += reward\n",
        "                game_rewards.append(reward)\n",
        "            if myMove!=None:\n",
        "                player1.remember(state, myMove[0], reward, next_state)\n",
        "    rewards.append(total_reward)\n",
        "    last_checking_rewards.append(sum(game_rewards[-last_rewards_len:]))\n",
        "end = timer()\n",
        "td = timedelta(seconds = int(end - start))\n",
        "print(f\"Elapsed time, {td}\")\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "figure(figsize = (120,30), dpi = 100)\n",
        "episodes = np.arange(1, len(rewards) + 1)\n",
        "plt.plot(episodes, rewards, label = \"Total rewards\")\n",
        "plt.plot(episodes, last_checking_rewards, label = \"Stepwise last 16 rewards\")\n",
        "plt.title('Togyz kumalak')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Rewards')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig('график.png')\n",
        "plt.show()\n",
        "print(\"Average reward\", sum(rewards)/n_episodes)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/MyDrive/Colab Notebooks/Togyzkumalak DQN')"
      ],
      "metadata": {
        "id": "__t52OAp2T0N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2b06a3e-0c15-43ad-fb78-629af8e92a6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    }
  ]
}